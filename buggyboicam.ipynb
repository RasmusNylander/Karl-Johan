{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from create_dataloader import make_dataloaders\n",
    "from medcam import medcam\n",
    "\n",
    "from model_picker import ModelType, get_model\n",
    "from monai.data.nifti_writer import nib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = \"./models\"\n",
    "DATA_PATH = \"./datasets/MNInSecT/\"\n",
    "BATCH_SIZE = 1\n",
    "assert BATCH_SIZE == 1\n",
    "\n",
    "model_type: ModelType = ModelType.ResNet18\n",
    "scale: float = 0.5\n",
    "assert scale in [0.25, 0.5, 1.0]\n",
    "\n",
    "model_string_id = f\"{model_type.name}_{str(int(scale*100)).zfill(3)}\"\n",
    "\n",
    "model_path = f\"{MODELS_ROOT}/{model_string_id}.pth\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "_, _, test_loader = make_dataloaders(num_workers=0, persistent_workers=False, data_path=DATA_PATH, batch_size=BATCH_SIZE, scale=scale)\n",
    "num_classes = test_loader.dataset.num_classes()\n",
    "\n",
    "model = get_model(model_type)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)['net'], strict=True)\n",
    "model = medcam.inject(model, return_attention=True, layer=\"auto\", label=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_volume(attention_map: Tensor, image: Tensor):\n",
    "    X, Y, Z = np.mgrid[0:(256*scale), 0:(128*scale), 0:(128*scale)]\n",
    "    image_values = image.squeeze()\n",
    "    attention_values = F.interpolate(attention_map.unsqueeze(dim=0).unsqueeze(dim=0), image_values.shape).squeeze()\n",
    "    # attention_values[attention_map < .5] = 0\n",
    "    attention_volume = go.Volume(\n",
    "        x=X.flatten(),\n",
    "        y=Y.flatten(),\n",
    "        z=Z.flatten(),\n",
    "        value=attention_values.flatten(),\n",
    "        isomin=0.0,\n",
    "        isomax=1.0,\n",
    "        opacity=0.05,  # needs to be small to see through all surfaces\n",
    "        surface_count=21,  # needs to be a large number for good volume rendering\n",
    "        colorscale='RdBu_r'\n",
    "    )\n",
    "    print(f\"Image size: {image_values.shape} Attention map size: {attention_values.shape}\")\n",
    "    image_volume = go.Volume(\n",
    "        x=X.flatten(),\n",
    "        y=Y.flatten(),\n",
    "        z=Z.flatten(),\n",
    "        value=image_values.flatten(),\n",
    "        isomin=0.0,\n",
    "        isomax=255.0,\n",
    "        opacity=0.1,  # needs to be small to see through all surfaces\n",
    "        surface_count=21,  # needs to be a large number for good volume rendering\n",
    "        colorscale='greys'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=(image_volume, attention_volume))\n",
    "    # fig = go.Figure(data=attention_volume)\n",
    "\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "current_image_id = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "layer = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from create_dataloader import Dataset\n",
    "assert BATCH_SIZE == 1\n",
    "\n",
    "image_batch, batch_labels = next(test_loader.__iter__())\n",
    "dataset: Dataset = test_loader.dataset\n",
    "image_name = dataset.get_name_of_image(current_image_id)\n",
    "\n",
    "attentionmap_paths = glob.glob(f\"./attention_maps/{model_string_id}/layer{layer}/{image_name}/*\")\n",
    "prediction_path = [path for path in attentionmap_paths if \"prediction\" in path][0]\n",
    "correct_path = [path for path in attentionmap_paths if \"correct\" in path][0]\n",
    "prediction_map = np.array(nib.load(prediction_path).dataobj).transpose(2, 0, 1)\n",
    "prediction_map = torch.from_numpy(prediction_map)\n",
    "plot_volume(prediction_map, image_batch)\n",
    "if prediction_path != correct_path:\n",
    "    correct_map = np.array(nib.load(correct_path).dataobj).transpose(2, 0, 1)\n",
    "    correct_map = torch.from_numpy(correct_map)\n",
    "    plot_volume(prediction_map, image_batch)\n",
    "current_image_id += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
