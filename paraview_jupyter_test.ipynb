{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tifffile import tifffile\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from create_dataloader import Augmentation, Dataset, DatasetScale, Label, MNInSecTVariant, SplitType\n",
    "from model_picker import ModelType, get_model_name\n",
    "\n",
    "MODELS_ROOT = \"./models\"\n",
    "DATA_PATH = \"./datasets/MNInSecT/\"\n",
    "ATTENTION_MAPS_ROOT = \"D:/attention_maps\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "model_type: ModelType = ModelType.SEResNet50\n",
    "scale: DatasetScale = DatasetScale.Scale50\n",
    "dataset_augmentation: Augmentation = Augmentation.Original"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_variant = MNInSecTVariant(dataset_augmentation, scale)\n",
    "dataset: Dataset = Dataset(MNInSecT_root=DATA_PATH, type=SplitType.Test, seed=69420, as_rgb=False, variant=dataset_variant)\n",
    "model_name = get_model_name(model_type, dataset_variant)\n",
    "\n",
    "@dataclass\n",
    "class AttentionMap:\n",
    "    model: ModelType\n",
    "    dataset: MNInSecTVariant\n",
    "    layer: int\n",
    "    image_name: str\n",
    "    label: Label\n",
    "\n",
    "    def fetch(self, root: str) -> Tensor:\n",
    "        attention_maps_path = os.path.join(root, get_model_name(self.model, self.dataset), f\"layer{self.layer}\", self.image_name, f\"{self.label.abbreviation}*.tif\")\n",
    "        attention_map_filename = glob.glob(attention_maps_path)[0]\n",
    "        attention_map = torch.from_numpy(tifffile.imread(attention_map_filename))\n",
    "        return attention_map\n",
    "\n",
    "\n",
    "def scale_image(image: Tensor, new_size) -> Tensor:\n",
    "    return F.interpolate(true_map.unsqueeze(dim=0).unsqueeze(dim=0), new_size)[0][0]\n",
    "\n",
    "def combine_image(image: Tensor, true_map: Tensor, predicted_map: Tensor):\n",
    "    true_map = scale_image(true_map, image.squeeze().shape)\n",
    "    true_map[true_map < 0.2] = 0\n",
    "\n",
    "    predicted_map = scale_image(predicted_map, image.squeeze().shape)\n",
    "    predicted_map[predicted_map < 0.2] = 0\n",
    "\n",
    "    image = image / image.max()\n",
    "    image = image[0]\n",
    "    image[image < 0.1] = 0\n",
    "\n",
    "    background = (predicted_map == 0) & (image == 0) & (true_map == 0)\n",
    "    opacity = torch.ones(image.shape)\n",
    "    opacity[background] = 1\n",
    "    opacity[image != 0] -= 0.2\n",
    "    opacity[true_map != 0] -= 0.1\n",
    "    opacity[predicted_map != 0] -= 0.1\n",
    "\n",
    "    combined = torch.stack([image, predicted_map, true_map, opacity], dim=-1).numpy()\n",
    "    combined_as_uint8 = (combined * 255).astype(np.uint8)\n",
    "    return combined_as_uint8\n",
    "\n",
    "def empty_folder(path: str) -> None:\n",
    "    contents = glob.glob(f\"{path}/*\")\n",
    "    for file in contents:\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "image_id = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "empty_folder(\"./combined\")\n",
    "\n",
    "image_name = dataset.get_name_of_image(image_id)\n",
    "true_label = Label.from_abbreviation(image_name[:2].upper())\n",
    "insect_image = dataset[image_id][0]\n",
    "\n",
    "for layer in range(1, 5):\n",
    "\n",
    "    true_label_path = os.path.join(ATTENTION_MAPS_ROOT, model_name, f\"layer{layer}\", image_name, f\"{true_label.abbreviation}*.tif\")\n",
    "    true_map_filename = glob.glob(true_label_path)[0]\n",
    "    true_map = torch.from_numpy(tifffile.imread(true_map_filename))\n",
    "\n",
    "    prediction_label_path = os.path.join(ATTENTION_MAPS_ROOT, model_name, f\"layer{layer}\", image_name, f\"*prediction*.tif\")\n",
    "    prediction_map_filename = glob.glob(prediction_label_path)[0]\n",
    "    if prediction_map_filename != true_map_filename:\n",
    "        prediction_map = torch.from_numpy(tifffile.imread(prediction_map_filename))\n",
    "    else:\n",
    "        prediction_map = torch.zeros(true_map.shape)\n",
    "\n",
    "    tifffile.imwrite(f\"./combined/{image_name[:6]}, {model_name} layer {layer}.tif\", combine_image(insect_image, true_map, prediction_map))\n",
    "image_id += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
